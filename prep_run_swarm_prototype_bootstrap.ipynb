{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T17:39:20.255455Z",
     "start_time": "2018-12-10T17:39:16.038277Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from sklearn_pandas import DataFrameMapper\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import normalized_mutual_info_score\n",
    "import itertools\n",
    "import getpass\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from collections import namedtuple\n",
    "from scipy import stats\n",
    "# Display more rows and get rid of the margins\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('max_colwidth',500)\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "\n",
    "# Display mulitiple values from each cell\n",
    "# https://stackoverflow.com/a/42476224\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from itertools import cycle\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define paths and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T17:39:20.260911Z",
     "start_time": "2018-12-10T17:39:20.258020Z"
    }
   },
   "outputs": [],
   "source": [
    "# Define the directories that we're going to read data from\n",
    "data_dir = Path('data')\n",
    "swarm_dir = Path('swarm_dir_bs2.0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T18:33:54.818013Z",
     "start_time": "2018-12-10T18:33:54.790052Z"
    }
   },
   "outputs": [],
   "source": [
    "def bal_samp(df, strata, balance, order, keys, n_splits=5, n_draws=100, drop_strata=False):\n",
    "    \"\"\"Balanced sampling across strata\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "        Dataframe from which you want to sample\n",
    "    strata: str or list of str\n",
    "        Name(s) of the column or columns that define the groups\n",
    "        from which you want a balanced sample\n",
    "    balance: str or list of str\n",
    "        Name(s) of the columns or columns containing the factors\n",
    "        you want to evenly sample across strata\n",
    "    order: str or list of str\n",
    "        Name(s) of the column whose distribution you want to preserve\n",
    "    keys: list of str\n",
    "        Name(s) of the column(s) that you will use to match\n",
    "        the output back to your original column\n",
    "    n_splits: int\n",
    "        Number of cross validation folds you want to create per draw\n",
    "    n_draws: int\n",
    "        Number of balanced samples of your dataset you want to create\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    draws_df: pandas.DataFrame\n",
    "        Dataframe with number of rows about equal to number of rows in df\n",
    "        and number of columns equal to n_draws + len(keys) + len(strata) + len(balance) + len(order).\n",
    "        Contains the crossfold labels for balanced sampling across the strata you've defined.\n",
    "    \"\"\"\n",
    "    # create dict of minimum count in each strata of each combination of balance factor\n",
    "    bal_dict = (df.groupby(strata + balance)[[keys[0]]]\n",
    "                  .nunique()\n",
    "                  .groupby(balance)\n",
    "                  .min()\n",
    "                  .to_dict('index'))\n",
    "    bal_dict = {k: v[keys[0]] for k, v in bal_dict.items()}\n",
    "\n",
    "    # Appologies for the disgusting nested loops\n",
    "    # For each draw, at each strata level, for each unique combin\n",
    "    draws_df = []\n",
    "    # For each draw\n",
    "    for nn in range(n_draws):\n",
    "        strat_df = []\n",
    "        # From each strata group\n",
    "        for x, gbdf in df.groupby(strata):\n",
    "            cvs_df = []\n",
    "            # from each unique combination of balance values\n",
    "            for bal_vals, num in bal_dict.items():\n",
    "                # create an index selecting the rows at those balance values\n",
    "                ind = np.ones((len(gbdf))).astype(bool)\n",
    "                for bcol, bv in zip(balance, bal_vals):\n",
    "                    ind = np.logical_and(ind, gbdf[bcol] == bv)\n",
    "                if ind.sum() == 0:\n",
    "                    if drop_strata:\n",
    "                        print(f\"There are no rows with {bal_vals} for {balance} in group {x}.\")\n",
    "                        continue\n",
    "                    else:\n",
    "                        raise ValueError(f\"There are no rows with {bal_vals} for {balance} in group {x}.\")\n",
    "                    \n",
    "                # draw a random sample of the group members\n",
    "                # that meet the balance criteria\n",
    "                # and sort them by the order values\n",
    "                bal_df = gbdf[ind].sample(\n",
    "                    n=num).sort_values(order).loc[:, keys]\n",
    "                # create a list of the cross validation values long enough to match\n",
    "                cv_inds = list(np.arange(n_splits))*((len(bal_df)//n_splits)+1)\n",
    "                bal_df['draw_%d' % nn] = cv_inds[:len(bal_df)]\n",
    "                # and append them to a list\n",
    "                cvs_df.append(bal_df)\n",
    "            # combine these lists to get all the rows for that strata\n",
    "            # and append them to create a list of selected rows from all the strata\n",
    "            strat_df.append(pd.concat(cvs_df).loc[:, ['draw_%d' % nn]])\n",
    "        # pull these all together to create the draws dataframe\n",
    "        draws_df.append(pd.concat(strat_df))\n",
    "    draws_df = pd.concat(draws_df, axis=1)\n",
    "    # Merge back in the indicator variables\n",
    "    draws_df = (df.loc[:,\n",
    "                       keys\n",
    "                       + strata\n",
    "                       + balance\n",
    "                       + order]\n",
    "                .merge(draws_df,\n",
    "                       right_index=True,\n",
    "                       left_index=True,\n",
    "                       how='left')\n",
    "                )\n",
    "    # make sure the shape is still ok\n",
    "    assert draws_df.shape[0] == df.shape[0]\n",
    "    assert draws_df.shape[1] == (n_draws\n",
    "                                 + len(keys)\n",
    "                                 + len(strata)\n",
    "                                 + len(balance)\n",
    "                                 + len(order))\n",
    "    return draws_df\n",
    "\n",
    "\n",
    "def gen_binned_perms(df, bin_levels, n_perms=1000, boot=False):\n",
    "    df = df.copy(deep=True)\n",
    "    permed_inds = []\n",
    "    permed_inds.append(df.index.values)\n",
    "    if not 'ind' in df.columns:\n",
    "        df['ind'] = df.index.values\n",
    "    else:\n",
    "        raise Exception\n",
    "    for pn in range(n_perms):\n",
    "        if not boot:\n",
    "            permed_inds.append(df.groupby(bin_levels).ind.transform(\n",
    "                np.random.permutation).values)\n",
    "        else:\n",
    "            permed_inds.append(df.sample(frac=1, replace=True).ind.values)\n",
    "    return permed_inds\n",
    "\n",
    "\n",
    "def get_cols(raw_df):\n",
    "    base_meta_cols = ['contrast', 'fmri_beta_gparc_numtrs', 'fmri_beta_gparc_tr', 'lmt_run',\n",
    "                      'mid_beta_seg_dof', 'task', 'collection_id', 'dataset_id', 'subjectkey',\n",
    "                      'src_subject_id', 'interview_date', 'interview_age',\n",
    "                      'gender', 'event_name', 'visit', 'rsfm_tr', 'eventname',\n",
    "                      'rsfm_nreps', 'rsfm_numtrs', 'pipeline_version',  'scanner_manufacturer_pd',\n",
    "                      'scanner_type_pd', 'mri_info_deviceserialnumber', 'magnetic_field_strength',\n",
    "                      'procdate', 'collection_title', 'promoted_subjectkey', 'study_cohort_name',\n",
    "                      'ehi_ss_score', '_merge', 'qc_ok', 'age_3mos', 'abcd_betnet02_id', 'fsqc_qc',\n",
    "                      'rsfmri_cor_network.gordon_visitid',\n",
    "                      'mrirscor02_id',  'site_id_l', 'mri_info_manufacturer',\n",
    "                      'mri_info_manufacturersmn', 'mri_info_deviceserialnumber',\n",
    "                      'mri_info_magneticfieldstrength', 'mri_info_softwareversion',\n",
    "                      'unique_scanner', 'tbl_id', 'tbl_visitid', \n",
    "                     'modality', 'metric', 'source_file', 'tbl_id_y', 'source_file_y', \n",
    "                     'run', 'mri_info_visitid', 'dmri_dti_postqc_qc',\n",
    "       'iqc_t2_ok_ser', 'iqc_mid_ok_ser', 'iqc_sst_ok_ser',\n",
    "       'iqc_nback_ok_ser', 'tfmri_mid_beh_perform.flag',\n",
    "       'tfmri_nback_beh_perform.flag', 'tfmri_sst_beh_perform.flag',\n",
    "       'tfmri_mid_all_beta_dof', 'tfmri_mid_all_sem_dof',\n",
    "       'tfmri_sst_all_beta_dof', 'tfmri_sst_all_sem_dof',\n",
    "       'tfmri_nback_all_beta_dof', 'tfmri_nback_all_sem_dof',\n",
    "       'mrif_score', 'mrif_hydrocephalus', 'mrif_herniation',\n",
    "       'mr_findings_ok', 'tbl_numtrs', 'tbl_dof', 'tbl_nvols', 'tbl_tr', 'tbl_subthresh.nvols',\n",
    "                     'rsfmri_cor_network.gordon_tr', 'rsfmri_cor_network.gordon_numtrs',\n",
    "       'rsfmri_cor_network.gordon_nvols',\n",
    "       'rsfmri_cor_network.gordon_subthresh.nvols',\n",
    "       'rsfmri_cor_network.gordon_subthresh.contig.nvols',\n",
    "       'rsfmri_cor_network.gordon_ntpoints', 'dataset_id_y', 'tbl_mean.motion', 'tbl_mean.trans', 'tbl_mean.rot',\n",
    "       'tbl_max.motion', 'tbl_max.trans', 'tbl_max.rot']\n",
    "    meta_cols = raw_df.columns[raw_df.columns.isin(base_meta_cols)].values\n",
    "    metric_cols = raw_df.columns[~raw_df.columns.isin(base_meta_cols)].values\n",
    "    return metric_cols, meta_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T18:34:51.782245Z",
     "start_time": "2018-12-10T18:34:51.771853Z"
    }
   },
   "outputs": [],
   "source": [
    "def big_sites(size_limit, df, metric_cols):\n",
    "    notnull_mask = np.logical_and(~(df.loc[:, metric_cols].isnull().sum(1).astype(bool)),\n",
    "                                  np.isfinite(df.loc[:, metric_cols]).sum(1).astype(bool))\n",
    "    notnull_mask = np.logical_and(\n",
    "        notnull_mask, df.unique_scanner.notnull())\n",
    "    notnull_mask = np.logical_and(notnull_mask, df.qc_ok == 1)\n",
    "    # for right now, just focus on sites with more than size_limit scans\n",
    "    scans_per_sn = df.loc[notnull_mask, :].groupby(['unique_scanner'])[\n",
    "        ['collection_id']].count().sort_values('collection_id', ascending=False)\n",
    "    big_sns = df.loc[df.unique_scanner.isin(scans_per_sn.query('collection_id > @size_limit').index)\n",
    "                     & notnull_mask, :]\n",
    "    scans_per_bigsn = big_sns.groupby('unique_scanner')[\n",
    "        ['collection_id']].count().sort_values('collection_id', ascending=False)\n",
    "\n",
    "    print(f\"Number of sites with more than {size_limit} scans:\", len(\n",
    "        big_sns.unique_scanner.unique()))\n",
    "    print(f\"Number of scans collected at sites with more than {size_limit} scans:\", len(\n",
    "        big_sns))\n",
    "    print(f\"Number of subjects collected at sites with more than {size_limit} scans:\",\n",
    "        big_sns.subjectkey.nunique())\n",
    "    scans_per_bigsn.query('collection_id > @size_limit')\n",
    "    return big_sns\n",
    "\n",
    "\n",
    "def drop_bad(df, metric_cols, val_range=None, std_range=None, pct_range=None, verbose=False):\n",
    "    if verbose:\n",
    "        df = df.copy()\n",
    "        df['cr'] = df.contrast + df.run\n",
    "        full_data_subs = len(pd.unique((df.groupby('subjectkey')[['cr']].nunique() == df.groupby('subjectkey')[['cr']].nunique().max()).query('cr').index.values))\n",
    "\n",
    "    notnull_mask = np.logical_and(~(df.loc[:, metric_cols].isnull().sum(1).astype(bool)),\n",
    "                                  np.isfinite(df.loc[:, metric_cols]).sum(1).astype(bool))\n",
    "    dropped_sum = (~notnull_mask).sum()\n",
    "    if verbose:        \n",
    "        dropped_subs = full_data_subs - len(pd.unique((df.loc[notnull_mask, :].groupby('subjectkey')[['cr']].nunique() == df.loc[notnull_mask, :].groupby('subjectkey')[['cr']].nunique().max()).query('cr').index.values))\n",
    "        print(f\"{dropped_sum} contrasts dropped for null data in one of the metrics\")\n",
    "        print(f\"{dropped_subs} subjects dropped for null data in one metric on one contrast\")\n",
    "        full_data_subs = len(pd.unique((df.loc[notnull_mask, :].groupby('subjectkey')[['cr']].nunique() == df.loc[notnull_mask, :].groupby('subjectkey')[['cr']].nunique().max()).query('cr').index.values))\n",
    "    assert notnull_mask.sum() > 0\n",
    "        \n",
    "    notnull_mask = np.logical_and(\n",
    "        notnull_mask, df.unique_scanner.notnull())\n",
    "    if verbose:\n",
    "        dropped_subs = full_data_subs - len(pd.unique((df.loc[notnull_mask, :].groupby('subjectkey')[['cr']].nunique() == df.loc[notnull_mask, :].groupby('subjectkey')[['cr']].nunique().max()).query('cr').index.values))\n",
    "        print(f\"{(~notnull_mask).sum() - dropped_sum} contrasts dropped for not having a scanner id\")\n",
    "        print(f\"{dropped_subs} subjects dropped for not having a scanner id\")\n",
    "        full_data_subs = len(pd.unique((df.loc[notnull_mask, :].groupby('subjectkey')[['cr']].nunique() == df.loc[notnull_mask, :].groupby('subjectkey')[['cr']].nunique().max()).query('cr').index.values))\n",
    "    dropped_sum = (~notnull_mask).sum()\n",
    "    assert notnull_mask.sum() > 0\n",
    "        \n",
    "    notnull_mask = np.logical_and(notnull_mask, df.qc_ok == 1)\n",
    "    if verbose:\n",
    "        dropped_subs = full_data_subs - len(pd.unique((df.loc[notnull_mask, :].groupby('subjectkey')[['cr']].nunique() == df.loc[notnull_mask, :].groupby('subjectkey')[['cr']].nunique().max()).query('cr').index.values))\n",
    "        print(f\"{(~notnull_mask).sum() - dropped_sum} contrasts dropped for failing QC\")\n",
    "        print(f\"{dropped_subs} subjects dropped because one run failed QC\")\n",
    "        full_data_subs = len(pd.unique((df.loc[notnull_mask, :].groupby('subjectkey')[['cr']].nunique() == df.loc[notnull_mask, :].groupby('subjectkey')[['cr']].nunique().max()).query('cr').index.values))\n",
    "    dropped_sum = (~notnull_mask).sum()\n",
    "    assert notnull_mask.sum() > 0\n",
    "\n",
    "    if val_range is not None:\n",
    "        notnull_mask = np.logical_and(notnull_mask, (np.abs(\n",
    "            df.loc[:, metric_cols]) < val_range).product(1).astype(bool))\n",
    "        if verbose:\n",
    "            dropped_subs = full_data_subs - len(pd.unique((df.loc[notnull_mask, :].groupby('subjectkey')[['cr']].nunique() == df.loc[notnull_mask, :].groupby('subjectkey')[['cr']].nunique().max()).query('cr').index.values))\n",
    "            print(f\"{(~notnull_mask).sum() - dropped_sum} contrasts dropped for exceeding value range\")\n",
    "            print(f\"{dropped_subs} subjects dropped because one contrast had a value exceeding value range\")\n",
    "            full_data_subs = len(pd.unique((df.loc[notnull_mask, :].groupby('subjectkey')[['cr']].nunique() == df.loc[notnull_mask, :].groupby('subjectkey')[['cr']].nunique().max()).query('cr').index.values))\n",
    "        dropped_sum = (~notnull_mask).sum()\n",
    "        assert notnull_mask.sum() > 0\n",
    "\n",
    "    if std_range is not None:\n",
    "        notnull_mask = np.logical_and(notnull_mask, (np.abs(\n",
    "            (df.loc[:, metric_cols] - df.loc[:, metric_cols].mean())/ df.loc[:, metric_cols].std()) < std_range).product(1).astype(bool))\n",
    "        if verbose:\n",
    "            dropped_subs = full_data_subs - len(pd.unique((df.loc[notnull_mask, :].groupby('subjectkey')[['cr']].nunique() == df.loc[notnull_mask, :].groupby('subjectkey')[['cr']].nunique().max()).query('cr').index.values))\n",
    "            print(f\"{(~notnull_mask).sum() - dropped_sum} contrasts dropped for exceeding variance range\")\n",
    "            print(f\"{dropped_subs} subjects dropped because one contrast had a value exceeding variance range\")\n",
    "            full_data_subs = len(pd.unique((df.loc[notnull_mask, :].groupby('subjectkey')[['cr']].nunique() == df.loc[notnull_mask, :].groupby('subjectkey')[['cr']].nunique().max()).query('cr').index.values))\n",
    "        dropped_sum = (~notnull_mask).sum()\n",
    "        assert notnull_mask.sum() > 0\n",
    "\n",
    "    if pct_range is not None:\n",
    "        top_pct = (100-pct_range)/100\n",
    "        bottom_pct = pct_range/100\n",
    "        notnull_mask = np.logical_and(notnull_mask, \n",
    "            (df.loc[:, metric_cols] <= df.loc[:, metric_cols].quantile([top_pct]).iloc[0, :])\n",
    "                .product(1)\n",
    "                .astype(bool))\n",
    "        notnull_mask = np.logical_and(notnull_mask, \n",
    "            (df.loc[:, metric_cols] >= df.loc[:, metric_cols].quantile([bottom_pct]).iloc[0, :])\n",
    "                .product(1)\n",
    "                .astype(bool))\n",
    "        if verbose:\n",
    "            dropped_subs = full_data_subs - len(pd.unique((df.loc[notnull_mask, :].groupby('subjectkey')[['cr']].nunique() == df.loc[notnull_mask, :].groupby('subjectkey')[['cr']].nunique().max()).query('cr').index.values))\n",
    "            print(f\"{(~notnull_mask).sum() - dropped_sum} contrasts dropped for exceeding percentile range\")\n",
    "            print(f\"{dropped_subs} subjects dropped because one contrast had a value exceeding percentile range\")\n",
    "            full_data_subs = len(pd.unique((df.loc[notnull_mask, :].groupby('subjectkey')[['cr']].nunique() == df.loc[notnull_mask, :].groupby('subjectkey')[['cr']].nunique().max()).query('cr').index.values))\n",
    "        dropped_sum = (~notnull_mask).sum()\n",
    "        assert notnull_mask.sum() > 0\n",
    "    return df.loc[notnull_mask, :].copy(deep=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_subjects(longtbl, modality, metric_range=None, **kwargs):\n",
    "    \n",
    "    for ii, mm in enumerate(longtbl.query('modality == @modality').metric.unique()):\n",
    "        metric_df = longtbl.loc[((longtbl.modality == modality) & (longtbl.metric == mm))].copy()\n",
    "        unused_cols = metric_df.columns[(metric_df.notnull().sum() == 0)].values\n",
    "        metric_df.drop(unused_cols, axis=1, inplace=True)\n",
    "\n",
    "        metric_cols, meta_cols = get_cols(metric_df)\n",
    "        \n",
    "        if metric_range is not None:\n",
    "            try:\n",
    "                metric_nb = drop_bad(metric_df, metric_cols, val_range = metric_range[mm], **kwargs)\n",
    "            except KeyError:\n",
    "                metric_nb = drop_bad(metric_df, metric_cols, **kwargs)\n",
    "        else:\n",
    "            metric_nb = drop_bad(metric_df, metric_cols, **kwargs)\n",
    "        if ii == 0:\n",
    "            full_subj = set(metric_nb.subjectkey.unique())\n",
    "        else:\n",
    "            full_subj = full_subj.intersection(metric_nb.subjectkey.unique())\n",
    "    \n",
    "    return full_subj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cmd_list(user, prange, modality, metrics, bundle=5, n_draws=25,\n",
    "                  run=None, task=None, contrast=None,\n",
    "                  out_base='/data/nielsond/abcd/nielson_abcd_2018/',\n",
    "                  out_subdir='release11',\n",
    "                  swarm_dir='/data/nielsond/abcd/nielson_abcd_2018/swarm_dir',\n",
    "                  image='/data/MLDSST/singularity_images/abcd_tmp-2018-04-26-a7f5b7d6a3b4.img',\n",
    "                  bootstrap=False,\n",
    "                  ymap_file = 'yfit.pkz',\n",
    "                  skip_completed=False):\n",
    "    if (modality =='tfmri') & ((task is None) | (contrast is None)):\n",
    "        raise ValueError('If modality is tfmri, then run, task and contrast mus be provided')\n",
    "    swarm_dir = Path(swarm_dir)\n",
    "    cmds = []\n",
    "    job_n = 0\n",
    "    rsync_in = ''#'rsync -avch /data/MLDSST/abcd/abcd_swarm_dir /lscratch/$SLURM_JOBID/; '\n",
    "    cmd = rsync_in\n",
    "    for pn in range(*prange):\n",
    "        for metric in metrics:\n",
    "            if bootstrap:\n",
    "                bs_or_pn = 'bs'\n",
    "            else:\n",
    "                bs_or_pn = 'pn'\n",
    "            if modality != 'tfmri':\n",
    "                out_path = f'{out_base}/{out_subdir}/{bs_or_pn}-{pn:04d}_{modality}_{metric}.pkz'\n",
    "            else:\n",
    "                out_path = f'{out_base}/{out_subdir}/{bs_or_pn}-{pn:04d}_crt_{task}_{contrast}.pkz'\n",
    "            if skip_completed and Path(out_path).exists():\n",
    "                continue\n",
    "            if not Path(out_path).exists():\n",
    "                if bootstrap:\n",
    "                    perm_file = swarm_dir/f'boots_{modality}.pkz'\n",
    "                else:\n",
    "                    perm_file = swarm_dir/f'perms_{modality}.pkz'\n",
    "                ymap_file = swarm_dir/ymap_file\n",
    "                if modality != 'tfmri':\n",
    "                    input_file = swarm_dir/f'{modality}_{metric}.pkz'\n",
    "                else:\n",
    "                    input_file = swarm_dir/f'crt_{task}_{contrast}_beta.pkz '\n",
    "\n",
    "\n",
    "                cmd_base = ('export SINGULARITY_BINDPATH=\"/gs3,/gs4,/gs5,/gs6,/gs7,/gs8,/gs9,/gs10,/gs11,/spin1,/scratch,/fdb,/data\";'\n",
    "                            + f' singularity exec -H ~/temp_for_singularity {image}'\n",
    "                            + f\" python {swarm_dir}/run_abcd_perm_new_draws.py {pn} {perm_file}\"\n",
    "                            + f\" {ymap_file} {input_file}\"\n",
    "                            + f\" {out_path} 30 --n_draws={n_draws}\")\n",
    "                if bootstrap:\n",
    "                    cmd_base += ' --bootstrap ;'\n",
    "                else:\n",
    "                    cmd_base += ' ;'\n",
    "                #rsync_out = f'rsync -ach {out_path} {local_path}; chown $USER:MLDSST {local_path}; '\n",
    "                cmd +=  cmd_base #+ rsync_out\n",
    "                job_n +=1\n",
    "                if job_n == bundle:\n",
    "                    cmds.append(cmd)\n",
    "                    cmd = rsync_in\n",
    "                    job_n = 0\n",
    "    return cmds\n",
    "\n",
    "#cmds = '\\n'.join(cmds)\n",
    "#swarm_file.write_text(cmds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep Dataframes for swarm jobs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rs-fMRI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_old = pd.read_pickle(data_dir/'con.pkz')\n",
    "\n",
    "con = pd.read_pickle(data_dir/'con_2.0.pkz')\n",
    "print(len(con))\n",
    "# drop the subjects that are in the 1.1 release\n",
    "# as well as those that don't pass qc, those without a entry for gender, and ambidexterous individiuals\n",
    "con = con.loc[(~con.subjectkey.isin(con_old.subjectkey.unique())), :]\n",
    "print(con.subjectkey.nunique())\n",
    "con = con.loc[(con.qc_ok == True ), :]\n",
    "print(con.subjectkey.nunique())\n",
    "con = con.loc[(con.gender.notnull()), :]\n",
    "print(con.subjectkey.nunique())\n",
    "con = con.loc[(con.ehi_ss_score ==1), :]\n",
    "print(con.subjectkey.nunique())\n",
    "con['age_6mos'] = (con['interview_age'] // 6) * 6\n",
    "\n",
    "# Drop lefties\n",
    "\n",
    "\n",
    "# Select the biggest scanner from each site\n",
    "biggest_scanners = (con\n",
    "                    .groupby(['site_id_l', 'unique_scanner'])[['subjectkey']]\n",
    "                    .nunique()\n",
    "                    .reset_index()\n",
    "                    .sort_values(['site_id_l', 'subjectkey'], ascending=[True, False])\n",
    "                    .groupby(['site_id_l'])\n",
    "                    .first().reset_index())\n",
    "con = con.loc[con.unique_scanner.isin(biggest_scanners.unique_scanner), :]\n",
    "print(len(con))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T18:35:59.397531Z",
     "start_time": "2018-12-10T18:35:58.873202Z"
    }
   },
   "outputs": [],
   "source": [
    "# Process connectivity data\n",
    "# If processed connectivity data doesn't exist, create it\n",
    "big_con_file = swarm_dir/'big_con.pkz'\n",
    "if not big_con_file.exists():\n",
    "    print(\"No processed connectivity data file found. Generating a new one.\")\n",
    "    #con = pd.read_pickle(data_dir/'con.pkz')\n",
    "    #con.loc[con.mri_info_manufacturersmn == 'DISCOVERY MR750', 'mri_info_manufacturersmn'] = 'AAAA DISCOVERY MR750'\n",
    "    con_metric_cols, con_meta_cols = get_cols(con)\n",
    "    big_con = big_sites(100, con, con_metric_cols)\n",
    "\n",
    "    big_con.to_pickle(big_con_file)\n",
    "else:\n",
    "    big_con = pd.read_pickle(big_con_file)\n",
    "\n",
    "# Look at counts in connectivity data\n",
    "big_con.groupby(['site_id_l',\n",
    "                 'mri_info_manufacturer',\n",
    "                 'mri_info_manufacturersmn',\n",
    "                 'mri_info_deviceserialnumber',\n",
    "                 'mri_info_softwareversion',\n",
    "                 'unique_scanner']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-10T18:42:17.461556Z",
     "start_time": "2018-12-10T18:41:58.570177Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load permutations or generate them\n",
    "perm_file = swarm_dir/'perms_con.pkz'\n",
    "if not perm_file.exists():\n",
    "    print(\"No connectivity permutation file found. Generating a new one.\")\n",
    "    # within bucket shuffle\n",
    "    big_con['age_3mos'] = (big_con['interview_age'] // 3) * 3\n",
    "    bin_levels = ['gender', 'ehi_ss_score', 'age_3mos']\n",
    "    con_perms = gen_binned_perms(big_con, bin_levels, n_perms=2000)\n",
    "    with open(perm_file.as_posix(), 'wb') as h:\n",
    "        pickle.dump(con_perms, h)\n",
    "else:\n",
    "    with open(perm_file.as_posix(), 'rb') as h:\n",
    "        con_perms = pickle.load(h)\n",
    "\n",
    "# If ymapper doesn't exist, create it\n",
    "# We're doing this outside the swarm to ensure that every worker has the same\n",
    "# encoding of the categories\n",
    "ymap_file = swarm_dir/'yfit_con.pkz'\n",
    "if not ymap_file.exists():\n",
    "    print(\"No connectivity ymapper file found. Generating a new one.\")\n",
    "\n",
    "    ymapper = DataFrameMapper([(\"unique_scanner\",preprocessing.LabelEncoder())])\n",
    "    yfit = ymapper.fit(big_con)\n",
    "    with open(swarm_dir/'yfit_con.pkz', 'wb') as h:\n",
    "        pickle.dump(yfit, h)\n",
    "        \n",
    "# Generate bootstraps\n",
    "boot_file = swarm_dir/'boots_con.pkz'\n",
    "\n",
    "if not boot_file.exists():\n",
    "    print(\"No connectivity bootstrap file found. Generating a new one.\")\n",
    "    # within bucket shuffle\n",
    "    bin_levels = []\n",
    "    con_boots = gen_binned_perms(big_con, bin_levels, n_perms=2000, boot=True)[1:]\n",
    "    with open(boot_file.as_posix(), 'wb') as h:\n",
    "        pickle.dump(con_boots, h)\n",
    "else:\n",
    "    with open(boot_file.as_posix(), 'rb') as h:\n",
    "        con_boots = pickle.load(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## task-based data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longtbl_old = pd.read_pickle(data_dir/'long_table.pkz')\n",
    "longtbl = pd.read_pickle(data_dir/'long_table_2.0.pkz')\n",
    "\n",
    "longtbl['unique_scanner'] = longtbl.site_id_l + '__' + longtbl.mri_info_deviceserialnumber + '__' + longtbl.mri_info_softwareversion\n",
    "\n",
    "print(longtbl.subjectkey.nunique())\n",
    "longtbl = longtbl.loc[(~longtbl.subjectkey.isin(longtbl_old.subjectkey.unique())), :]\n",
    "print(longtbl.subjectkey.nunique())\n",
    "longtbl = longtbl.loc[(longtbl.ehi_ss_score == 1), :]\n",
    "print(longtbl.subjectkey.nunique())\n",
    "longtbl = longtbl.loc[longtbl.gender.notnull(), :]\n",
    "print(longtbl.subjectkey.nunique())\n",
    "\n",
    "tb_b = longtbl.loc[((longtbl.modality == 'tfmri') & (longtbl.metric == 'beta') & (longtbl.run != 'all'))].copy().reset_index(drop=True)\n",
    "\n",
    "unused_cols = tb_b.groupby(['task','run','metric','contrast']).count().sum().index[tb_b.groupby(['task','run','metric', 'contrast']).count().sum() < 7000]\n",
    "\n",
    "tb_b.drop(unused_cols, axis=1, inplace=True)\n",
    "\n",
    "metric_cols, meta_cols = get_cols(tb_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longtbl.groupby(['task','run','metric','contrast'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ymapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If ymapper doesn't exist, create it\n",
    "# We're doing this outside the swarm to ensure that every worker has the same\n",
    "# encoding of the categories\n",
    "ymap_file = swarm_dir/f'yfit.pkz'\n",
    "if not ymap_file.exists():\n",
    "    ymapper = DataFrameMapper([(\"unique_scanner\",preprocessing.LabelEncoder())])\n",
    "    yfit = ymapper.fit(longtbl[longtbl.unique_scanner.notnull()])\n",
    "    with open(ymap_file, 'wb') as h:\n",
    "        pickle.dump(yfit, h)\n",
    "else:\n",
    "    with open(ymap_file, 'rb') as h:\n",
    "        yfit = pickle.load(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with single run per subject\n",
    "# For task based analysis limit to only subjects that have data across all tasks and runs\n",
    "tb_b_nb = drop_bad(tb_b, metric_cols, verbose=True, pct_range=0.25)\n",
    "tb_b_nb['cr'] = tb_b_nb.contrast + tb_b_nb.run\n",
    "# Get the first complete run for each task for each subject\n",
    "contrasts_per_run = pd.DataFrame(tb_b_nb\n",
    "                                 .groupby(['subjectkey', 'task', 'run'])\n",
    "                                 .collection_id.count()\n",
    "                                 .reset_index()\n",
    "                                 .groupby(['task', 'run'])\n",
    "                                 .collection_id.max()\n",
    "                                ).rename(columns={'collection_id':'max_ncontrasts'}).reset_index()\n",
    "subj_cpr = pd.DataFrame(tb_b_nb.groupby(['subjectkey', 'task', 'run'])\n",
    "                        .collection_id\n",
    "                        .count()\n",
    "                       ).rename(columns={'collection_id':'ncontrasts'}).reset_index()\n",
    "subj_cpr = subj_cpr.merge(contrasts_per_run, how='left', on=['task', 'run'])\n",
    "subj_cpr = subj_cpr.loc[subj_cpr.ncontrasts == subj_cpr.max_ncontrasts].sort_values(['subjectkey', 'task', 'run'])\n",
    "tb_b_nb = (subj_cpr\n",
    "             .groupby(['subjectkey', 'task'])[['run']].first()\n",
    "             .reset_index()\n",
    "             .merge(tb_b_nb, on=['subjectkey', 'task', 'run'], how='left'))\n",
    "full_data_subs = pd.unique((tb_b_nb.groupby('subjectkey')[['cr']].nunique() == tb_b_nb.groupby('subjectkey')[['cr']].nunique().max()).query('cr').index.values)\n",
    "tb_b_full = tb_b_nb.loc[tb_b_nb.subjectkey.isin(full_data_subs), :]\n",
    "\n",
    "\n",
    "\n",
    "#winsorize values\n",
    "# pct_range = 0.25\n",
    "# top_pct = (100-pct_range)/100\n",
    "# bottom_pct = pct_range/100\n",
    "# tb_b_full.loc[:, metric_cols] = tb_b_full.loc[:, metric_cols].clip(tb_b_full.quantile(bottom_pct), tb_b_full.quantile(top_pct), axis=1)\n",
    "\n",
    "print(f'nsubs = {tb_b_full.subjectkey.nunique()}')\n",
    "big_crs = []\n",
    "for x,df in tb_b_full.query('run != \"all\"').groupby(['contrast']):\n",
    "    tmp = big_sites(50, df, metric_cols).sort_values('subjectkey').reset_index()\n",
    "    assert len(tmp) > 0\n",
    "    big_crs.append(tmp)\n",
    "big_crs[-1].groupby(['mri_info_manufacturer',\n",
    "                      'mri_info_manufacturersmn','unique_scanner']).count()\n",
    "assert np.array([(big_crs[0].subjectkey == bc.subjectkey).all() for bc in big_crs]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for crt in big_crs:\n",
    "    assert (crt.index == big_crs[0].index).all()\n",
    "    task = crt.task.unique()[0]\n",
    "    contrast = '-'.join(crt.contrast.unique()[0].split())\n",
    "    crt_file = swarm_dir/f'crt_{task}_{contrast}_beta.pkz'\n",
    "    if not crt_file.exists():\n",
    "        crt.to_pickle(crt_file)\n",
    "\n",
    "# Load permutations or generate them\n",
    "crt_perm_file = swarm_dir/'perms_tfmri.pkz'\n",
    "if not crt_perm_file.exists():\n",
    "    print(\"No task based permutation file found. Generating a new one.\")\n",
    "    # within bucket shuffle\n",
    "    big_crs[0]['age_6mos'] = (big_crs[0]['interview_age'] // 6) * 6\n",
    "    bin_levels = ['gender', 'ehi_ss_score', 'age_6mos']\n",
    "    crt_perms = gen_binned_perms(big_crs[0], bin_levels, n_perms=2000)\n",
    "    with open(crt_perm_file, 'wb') as h:\n",
    "        pickle.dump(crt_perms, h)\n",
    "else:\n",
    "    with open(crt_perm_file.as_posix(), 'rb') as h:\n",
    "        crt_perms = pickle.load(h)\n",
    "\n",
    "\n",
    "# Generate bootstraps\n",
    "crt_boot_file = swarm_dir/'boots_tfmri.pkz'\n",
    "\n",
    "if not crt_boot_file.exists():\n",
    "    print(\"No task based bootstrap file found. Generating a new one.\")\n",
    "    # within bucket shuffle\n",
    "    bin_levels = []\n",
    "    crt_boots = gen_binned_perms(big_crs[0], bin_levels, n_perms=2000, boot=True)[1:]\n",
    "    with open(crt_boot_file.as_posix(), 'wb') as h:\n",
    "        pickle.dump(crt_boots, h)\n",
    "else:\n",
    "    with open(crt_boot_file.as_posix(), 'rb') as h:\n",
    "        crt_boots = pickle.load(h)\n",
    "        \n",
    "\n",
    "# If ymapper doesn't exist, create it\n",
    "# We're doing this outside the swarm to ensure that every worker has the same\n",
    "# encoding of the categories\n",
    "crt_ymap_file = swarm_dir/'yfit_crt_beta.pkz'\n",
    "if not crt_ymap_file.exists():\n",
    "    crt_ymapper = DataFrameMapper([(\"unique_scanner\",preprocessing.LabelEncoder())])\n",
    "    crt_yfit = crt_ymapper.fit(big_crs[0])\n",
    "    with open(crt_ymap_file, 'wb') as h:\n",
    "        pickle.dump(crt_yfit, h)\n",
    "else:\n",
    "    with open(crt_ymap_file, 'rb') as h:\n",
    "        crt_yfit = pickle.load(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write swarm files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## rs-fMRI data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Clean up paths here\n",
    "from pathlib import Path\n",
    "swarm_file = Path('swarm_con')\n",
    "cmds = []\n",
    "for pn in range(600,1000):\n",
    "    out_path = '/data/MBDU/nielsond/abcd_combat/nielsond_abcd_2018_dn_working/bootstrap/single_runs/bs-%04d_con_unadj.pkz'%(pn)\n",
    "    cmd_base = ( 'export SINGULARITY_BINDPATH=\"/gs3,/gs4,/gs5,/gs6,/gs7,/gs8,/gs9,/gs10,/gs11,/spin1,/scratch,/fdb,/data\";'\n",
    "                + ' singularity exec -H ~/temp_for_singularity /data/MLDSST/singularity_images/nielsond_abcd_new-2019-04-23-76484f889f81.simg'\n",
    "                + f\" python /data/MBDU/nielsond/abcd_combat/nielsond_abcd_2018_dn_working/swarm_dir/run_abcd_perm_new_draws.py {pn} /data/MBDU/nielsond/abcd_combat/nielsond_abcd_2018_dn_working/swarm_dir_bs2.0/boots_con.pkz \"\n",
    "                + f\"/data/MBDU/nielsond/abcd_combat/nielsond_abcd_2018_dn_working/swarm_dir_bs2.0/yfit_con.pkz /data/MBDU/nielsond/abcd_combat/nielsond_abcd_2018_dn_working/swarm_dir_bs2.0/big_con.pkz \"\n",
    "                + f\"{out_path} 30 --n_draws=1 --do_ab --bootstrap; \")\n",
    "    cmds.append(cmd_base)\n",
    "\n",
    "n_draws = 25\n",
    "for pn in range(600,1000):\n",
    "\n",
    "    out_path = '/data/MBDU/nielsond/abcd_combat/nielsond_abcd_2018_dn_working/bootstrap/single_runs/pn-%04d_con_unadj.pkz'%(pn)\n",
    "    cmd_base = ( 'export SINGULARITY_BINDPATH=\"/gs3,/gs4,/gs5,/gs6,/gs7,/gs8,/gs9,/gs10,/gs11,/spin1,/scratch,/fdb,/data\";'\n",
    "                + ' singularity exec -H ~/temp_for_singularity /data/MLDSST/singularity_images//nielsond_abcd_new-2019-04-23-76484f889f81.simg'\n",
    "                + f\" python /data/MBDU/nielsond/abcd_combat/nielsond_abcd_2018_dn_working/swarm_dir/run_abcd_perm_new_draws.py {pn} /data/MBDU/nielsond/abcd_combat/nielsond_abcd_2018_dn_working/swarm_dir_bs2.0/perms_con.pkz \"\n",
    "                + f\"/data/MBDU/nielsond/abcd_combat/nielsond_abcd_2018_dn_working/swarm_dir_bs2.0/yfit_con.pkz /data/MBDU/nielsond/abcd_combat/nielsond_abcd_2018_dn_working/swarm_dir_bs2.0/big_con.pkz \"\n",
    "                + f\"{out_path} 30 --n_draws={n_draws} --do_ab; \")\n",
    "    cmds.append(cmd_base)\n",
    "\n",
    "    \n",
    "    \n",
    "cmds = '\\n'.join(cmds)\n",
    "swarm_file.write_text(cmds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmds.split('\\n')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, outside of jupyter, run the swarm file:\n",
    "!swarm -f swarm_con -g 64 -t 32 --logdir swarm_log --time 4:00:00 --partition=norm,quick --gres=lscratch:20 --module=singularity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Task-based data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crt_combinations = longtbl.query('modality == \"tfmri\" & metric == \"beta\"').groupby(['task','contrast']).size().reset_index().drop(0,axis=1).to_records(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{user_cmds['nielsond'][0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crt_combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "users=['nielsond']\n",
    "user=users[0]\n",
    "user_cmds = {u:[] for u in users}\n",
    "users = cycle(users)\n",
    "pstep = 100\n",
    "pranges = [(0,101)]\n",
    "i = 0\n",
    "loop_n = 4\n",
    "modality='tfmri'\n",
    "out_base = '/data/MBDU/nielsond/abcd_combat/nielsond_abcd_2018_dn_working'\n",
    "out_subdir = 'bootstrap/single_runs'\n",
    "swarm_dir = '/data/MBDU/nielsond/abcd_combat/nielsond_abcd_2018_dn_working/swarm_dir_bs2.0'\n",
    "image = '/data/MLDSST/singularity_images/nielsond_abcd_new-2019-04-23-76484f889f81.simg'\n",
    "for pr in pranges:\n",
    "    cmds =[]\n",
    "    for task, contrast in crt_combinations:\n",
    "        cmds.extend(make_cmd_list(user, pr, modality, ['beta'], bundle=1, \n",
    "                                  task=task, contrast=contrast,\n",
    "                                  out_base=out_base, out_subdir=out_subdir,\n",
    "                                  swarm_dir=swarm_dir, bootstrap=True,\n",
    "                                  n_draws=1, image=image, skip_completed=True))\n",
    "        \n",
    "#         cmds.extend(make_cmd_list(user, pr, modality, ['beta'], bundle=1, \n",
    "#                                   task=task, contrast=contrast,\n",
    "#                                   out_base=out_base, out_subdir=out_subdir,\n",
    "#                                   swarm_dir=swarm_dir, image=image, skip_completed=True))\n",
    "        \n",
    "    user = next(users)\n",
    "    jobs_per_swarm = 1000\n",
    "    swarm_files = []\n",
    "    for ii in range(0, len(cmds), jobs_per_swarm):\n",
    "        swarm_file = Path('/data/MBDU/nielsond/abcd_combat/nielsond_abcd_2018_dn_working/swarm_dir_bs2.0/swarm_cmds/swarm_test_%s_%d_%d_%s_%d_11'%(modality, pr[0],ii,user, loop_n))\n",
    "        swarm_files.append(swarm_file.as_posix())\n",
    "        swarm_file.write_text('\\n'.join(cmds[ii:ii+jobs_per_swarm]))\n",
    "        swarm_cmd = f'swarm -f {swarm_file} -g 20 -t auto --logdir ${{USER}}_swarm_log --time 8:00:00 --partition=norm --gres=lscratch:30 --module singularity --exclusive --sbatch \"--constraint=core16\"'\n",
    "        user_cmds[user].append(swarm_cmd)\n",
    "        print(len(cmds[ii:ii+jobs_per_swarm]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(cmds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo = make_cmd_list(user, pr, modality, ['beta'], bundle=1, \n",
    "                                  task=task, contrast=contrast,\n",
    "                                  out_base=out_base, out_subdir=out_subdir,\n",
    "                                  swarm_dir=swarm_dir, bootstrap=True,\n",
    "                                  n_draws=1, image=image, skip_completed=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pr in pranges:\n",
    "    cmds =[]\n",
    "    for task, contrast in crt_combinations:\n",
    "        cmds.extend(make_cmd_list(user, pr, modality, ['beta'], bundle=1, \n",
    "                                  task=task, contrast=contrast,\n",
    "                                  out_base=out_base, out_subdir=out_subdir,\n",
    "                                  swarm_dir=swarm_dir, bootstrap=True,\n",
    "                                  n_draws=1, image=image, skip_completed=True))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(crt_combinations) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([ss for ss in cmds if 'pn-' in ss ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_cmds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!swarm -f /data/MBDU/nielsond/abcd_combat/nielsond_abcd_2018_dn_working/swarm_dir_bs2.0/swarm_cmds/swarm_test_tfmri_0_0_nielsond_4_11 -g 20 -t auto --logdir ${USER}_swarm_log --time 1:00:00 --partition=norm,quick --gres=lscratch:30 --module singularity --exclusive --sbatch \"--constraint=core16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for user,items in user_cmds.items():\n",
    "    if user == getpass.getuser():\n",
    "        for cmd in items[1:]:\n",
    "            notsubmitted = True\n",
    "            while notsubmitted:\n",
    "                print(cmd)\n",
    "                !{cmd}\n",
    "                if _exit_code == 0:\n",
    "                    notsubmitted=False\n",
    "                else:\n",
    "                    print(\"wait an hour and then try again\")\n",
    "                    time.sleep(3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users=['nielsond']\n",
    "user=users[0]\n",
    "user_cmds = {u:[] for u in users}\n",
    "users = cycle(users)\n",
    "pstep = 100\n",
    "pranges = [(0,101)]\n",
    "i = 0\n",
    "loop_n = 4\n",
    "modality='tfmri'\n",
    "out_base = '/data/MBDU/nielsond/abcd_combat/nielsond_abcd_2018_dn_working'\n",
    "out_subdir = 'bootstrap/single_runs'\n",
    "swarm_dir = '/data/MBDU/nielsond/abcd_combat/nielsond_abcd_2018_dn_working/swarm_dir_bs2.0'\n",
    "image = '/data/MLDSST/singularity_images/nielsond_abcd_new-2019-04-23-76484f889f81.simg'\n",
    "for pr in pranges:\n",
    "    cmds =[]\n",
    "    for task, contrast in crt_combinations:\n",
    "#         cmds.extend(make_cmd_list(user, pr, modality, ['beta'], bundle=1, \n",
    "#                                   task=task, contrast=contrast,\n",
    "#                                   out_base=out_base, out_subdir=out_subdir,\n",
    "#                                   swarm_dir=swarm_dir, bootstrap=True,\n",
    "#                                   n_draws=1, image=image, skip_completed=True))\n",
    "        \n",
    "        cmds.extend(make_cmd_list(user, pr, modality, ['beta'], bundle=1, \n",
    "                                  task=task, contrast=contrast,\n",
    "                                  out_base=out_base, out_subdir=out_subdir,\n",
    "                                  swarm_dir=swarm_dir, image=image, skip_completed=True))\n",
    "        \n",
    "    user = next(users)\n",
    "    jobs_per_swarm = 1000\n",
    "    swarm_files = []\n",
    "    for ii in range(0, len(cmds), jobs_per_swarm):\n",
    "        swarm_file = Path('/data/MBDU/nielsond/abcd_combat/nielsond_abcd_2018_dn_working/swarm_dir_bs/swarm_cmds/swarm_test_pns_%s_%d_%d_%s_%d_11'%(modality, pr[0],ii,user, loop_n))\n",
    "        swarm_files.append(swarm_file.as_posix())\n",
    "        swarm_file.write_text('\\n'.join(cmds[ii:ii+jobs_per_swarm]))\n",
    "        swarm_cmd = f'swarm -f {swarm_file} -g 20 -t auto --logdir ${{USER}}_swarm_log --time 8:00:00 --partition=norm --gres=lscratch:30 --module singularity --exclusive --sbatch \"--constraint=core16\"'\n",
    "        user_cmds[user].append(swarm_cmd)\n",
    "        print(len(cmds[ii:ii+jobs_per_swarm]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(cmds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for user,items in user_cmds.items():\n",
    "    if user == getpass.getuser():\n",
    "        for cmd in items:\n",
    "            notsubmitted = True\n",
    "            while notsubmitted:\n",
    "                print(cmd)\n",
    "                !{cmd}\n",
    "                if _exit_code == 0:\n",
    "                    notsubmitted=False\n",
    "                else:\n",
    "                    print(\"wait an hour and then try again\")\n",
    "                    time.sleep(3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code below here is not used for preregisterred analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# structural mri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longtbl = pd.read_pickle(data_dir/'long_table.pkz')\n",
    "longtbl['unique_scanner'] = longtbl.site_id_l + '__' + longtbl.mri_info_deviceserialnumber + '__' + longtbl.mri_info_softwareversion\n",
    "longtbl['age_3mos'] = (longtbl['interview_age'] // 3) * 3\n",
    "longtbl.drop(['subcort.aseg__wm.hypointensities.rh', 'subcort.aseg__wm.hypointensities.lh'], axis=1, inplace=True)\n",
    "modality = 'smri'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_subjs = get_full_subjects(longtbl, modality)\n",
    "\n",
    "metric_cols={}\n",
    "for mm in longtbl.query('modality == @modality').metric.unique():\n",
    "    metric_df = longtbl.loc[((longtbl.modality == modality) & (longtbl.metric == mm))].copy()\n",
    "    unused_cols = metric_df.columns[(metric_df.notnull().sum() == 0)].values\n",
    "    metric_df.drop(unused_cols, axis=1, inplace=True)\n",
    "\n",
    "    metric_cols, meta_cols = get_cols(metric_df)\n",
    "    \n",
    "    metric_full = metric_df.loc[metric_df.subjectkey.isin(full_subjs), :]\n",
    "    \n",
    "    metric_bs = big_sites(100, metric_full, metric_cols).sort_values('subjectkey').reset_index(drop=True)\n",
    "    \n",
    "    metric_file = swarm_dir/f'{modality}_{mm}.pkz'\n",
    "    if True:#not metric_file.exists():\n",
    "            metric_bs.to_pickle(metric_file)\n",
    "\n",
    "    # Load permutations or generate them\n",
    "    modality_perm_file = swarm_dir/f'perms_{modality}.pkz'\n",
    "    if not modality_perm_file.exists():\n",
    "        print(\"No permutation file found. Generating a new one.\")\n",
    "        # within bucket shuffle\n",
    "        bin_levels = ['gender', 'ehi_ss_score', 'age_3mos']\n",
    "        modality_perms = gen_binned_perms(metric_bs, bin_levels, n_perms=2000)\n",
    "        with open(modality_perm_file, 'wb') as h:\n",
    "            pickle.dump(modality_perms, h)\n",
    "    else:\n",
    "        with open(modality_perm_file.as_posix(), 'rb') as h:\n",
    "            modality_perms = pickle.load(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#users=['nielsond']\n",
    "#user_cmds = {u:[] for u in users}\n",
    "#users = cycle(users)\n",
    "#pstep = 100\n",
    "#pranges = [(100, 200)]\n",
    "i = 0\n",
    "loop_n = 4\n",
    "modality = 'smri'\n",
    "for pr in pranges:\n",
    "    user = next(users)\n",
    "    cmds = make_cmd_list(user, pr, modality, longtbl.query('modality == @modality').metric.unique(), bundle=1,\n",
    "                        out_base='/data/MLDSST/abcd/release11',\n",
    "                        swarm_dir='/data/MLDSST/abcd/abcd_swarm_dir')\n",
    "    jobs_per_swarm = 1000\n",
    "    swarm_files = []\n",
    "    for ii in range(0, len(cmds), jobs_per_swarm):\n",
    "        swarm_file = Path('/data/MLDSST/abcd/abcd_swarm_dir/swarm_test_%s_%d_%d_%s_%d_11'%(modality, pr[0],ii,user, loop_n))\n",
    "        swarm_files.append(swarm_file.as_posix())\n",
    "        swarm_file.write_text('\\n'.join(cmds[ii:ii+jobs_per_swarm]))\n",
    "        swarm_cmd = f'swarm -f {swarm_file} -g 12 -t auto --logdir ${{USER}}_swarm_log --time 2:30:00 --partition=quick,norm --gres=lscratch:30 --module singularity --exclusive --sbatch \"--constraint=core16\"'\n",
    "        user_cmds[user].append(swarm_cmd)\n",
    "        print(len(cmds[ii:ii+jobs_per_swarm]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user,items in user_cmds.items():\n",
    "    if user == getpass.getuser():\n",
    "        for cmd in items:\n",
    "            print(cmd)\n",
    "            ! {cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "metric='t2w.white02'\n",
    "roi='cort.desikan__caudalmiddlefrontal.lh'\n",
    "idf = longtbl.query('metric == @metric').loc[:, ['mri_info_manufacturer', roi,'interview_age']]\n",
    "g_dat=idf.query(\"mri_info_manufacturer == 'GE MEDICAL SYSTEMS'\").loc[idf[roi].notnull(), roi]\n",
    "s_dat=idf.query(\"mri_info_manufacturer == 'SIEMENS'\").loc[idf[roi].notnull(), roi]\n",
    "p_dat=idf.query(\"mri_info_manufacturer == 'Philips Medical Systems'\").loc[idf[roi].notnull(), roi]\n",
    "\n",
    "sns.distplot(g_dat, label='GE')\n",
    "sns.distplot(s_dat, label='Philips')\n",
    "sns.distplot(p_dat, label='Siemens')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dmri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "longtbl = pd.read_pickle(data_dir/'long_table.pkz')\n",
    "longtbl['unique_scanner'] = longtbl.site_id_l + '__' + longtbl.mri_info_deviceserialnumber + '__' + longtbl.mri_info_softwareversion\n",
    "longtbl['age_3mos'] = (longtbl['interview_age'] // 3) * 3\n",
    "modality = 'dmri'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "full_subjs = get_full_subjects(longtbl, modality, metric_range={'rsi.n0.gwc':4})\n",
    "\n",
    "metric_cols={}\n",
    "for mm in longtbl.query('modality == @modality').metric.unique():\n",
    "    metric_df = longtbl.loc[((longtbl.modality == modality) & (longtbl.metric == mm))].copy()\n",
    "    unused_cols = metric_df.columns[(metric_df.notnull().sum() == 0)].values\n",
    "    metric_df.drop(unused_cols, axis=1, inplace=True)\n",
    "\n",
    "    metric_cols, meta_cols = get_cols(metric_df)\n",
    "    \n",
    "    metric_full = metric_df.loc[metric_df.subjectkey.isin(full_subjs), :]\n",
    "    \n",
    "    metric_bs = big_sites(100, metric_full, metric_cols).sort_values('subjectkey').reset_index(drop=True)\n",
    "    \n",
    "    metric_file = swarm_dir/f'{modality}_{mm}.pkz'\n",
    "    if True:#not metric_file.exists():\n",
    "        metric_bs.to_pickle(metric_file)\n",
    "\n",
    "    # Load permutations or generate them\n",
    "    modality_perm_file = swarm_dir/f'perms_{modality}.pkz'\n",
    "    if not modality_perm_file.exists():\n",
    "        print(\"No permutation file found. Generating a new one.\")\n",
    "        # within bucket shuffle\n",
    "        bin_levels = ['gender', 'ehi_ss_score', 'age_3mos']\n",
    "        modality_perms = gen_binned_perms(metric_bs, bin_levels, n_perms=2000)\n",
    "        with open(modality_perm_file, 'wb') as h:\n",
    "            pickle.dump(modality_perms, h)\n",
    "    else:\n",
    "        with open(modality_perm_file.as_posix(), 'rb') as h:\n",
    "            modality_perms = pickle.load(h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users=['nielsond']\n",
    "#user_cmds = {u:[] for u in users}\n",
    "users = cycle(users)\n",
    "pstep = 100\n",
    "pranges = [(0,101)]\n",
    "i = 0\n",
    "loop_n = 4\n",
    "modality = 'dmri'\n",
    "for pr in pranges:\n",
    "    user = next(users)\n",
    "    cmds = make_cmd_list(user, pr, modality, longtbl.query('modality == @modality').metric.unique(), bundle=1)\n",
    "    #cmds = make_cmd_list(user, pr, modality, ['rsi.n0.gwc'], bundle=1)\n",
    "    jobs_per_swarm = 1000\n",
    "    swarm_files = []\n",
    "    for ii in range(0, len(cmds), jobs_per_swarm):\n",
    "        swarm_file = Path('/data/nielsond/abcd/nielson_abcd_2018/swarm_dir/swarm_test_%s_%d_%d_%s_%d_11'%(modality, pr[0],ii,user, loop_n))\n",
    "        swarm_files.append(swarm_file.as_posix())\n",
    "        swarm_file.write_text('\\n'.join(cmds[ii:ii+jobs_per_swarm]))\n",
    "        swarm_cmd = f'swarm -f {swarm_file} -g 12 -t auto --logdir ${{USER}}_swarm_log --time 2:30:00 --partition=quick,norm --gres=lscratch:30 --module singularity --exclusive --sbatch \"--constraint=core16\"'\n",
    "        user_cmds[user].append(swarm_cmd)\n",
    "        print(len(cmds[ii:ii+jobs_per_swarm]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "export SINGULARITY_BINDPATH=\"/gs3,/gs4,/gs5,/gs6,/gs7,/gs8,/gs9,/gs10,/gs11,/spin1,/scratch,/fdb,/data\"; singularity exec -H ~/temp_for_singularity /data/ML\n",
    "DSST/singularity_images/abcd_tmp-2018-04-26-a7f5b7d6a3b4.img python /data/MBDU/nielsond/abcd_combat/nielsond_abcd_2018_dn_working/swarm_dir_bs/run_abcd_perm\n",
    "_new_draws.py 0 /data/MBDU/nielsond/abcd_combat/nielsond_abcd_2018_dn_working/swarm_dir_bs/perms_tfmri.pkz /data/MBDU/nielsond/abcd_combat/nielsond_abcd_201\n",
    "8_dn_working/swarm_dir_bs/yfit.pkz /data/MBDU/nielsond/abcd_combat/nielsond_abcd_2018_dn_working/swarm_dir_bs/crt_mid_antic.large.loss.vs.neutral_run1_beta.\n",
    "pkz  /data/MBDU/nielsond/abcd_combat/nielsond_abcd_2018_dn_working/bootstrap_test/single_runs/pn-0000_crt_mid_antic.large.loss.vs.neutral_run1.pkz 30 --n_dr\n",
    "aws=25 ;export SINGULARITY_BINDPATH=\"/gs3,/gs4,/gs5,/gs6,/gs7,/gs8,/gs9,/gs10,/gs11,/spin1,/scratch,/fdb,/data\"; singularity exec -H ~/temp_for_singularity \n",
    "/data/MLDSST/singularity_images/abcd_tmp-2018-04-26-a7f5b7d6a3b4.img python /data/MBDU/nielsond/abcd_combat/nielsond_abcd_2018_dn_working/swarm_dir_bs/run_a\n",
    "bcd_perm_new_draws.py 0 /data/MBDU/nielsond/abcd_combat/nielsond_abcd_2018_dn_working/swarm_dir_bs/perms_tfmri.pkz /data/MBDU/nielsond/abcd_combat/nielsond_\n",
    "abcd_2018_dn_working/swarm_dir_bs/yfit.pkz /data/MBDU/nielsond/abcd_combat/nielsond_abcd_2018_dn_working/swarm_dir_bs/crt_mid_antic.large.loss.vs.neutral_ru\n",
    "n1_beta.pkz  /data/MBDU/nielsond/abcd_combat/nielsond_abcd_2018_dn_working/bootstrap_test/single_runs/pn-0000_crt_mid_antic.large.loss.vs.neutral_run1.pkz 3\n",
    "0 --n_draws=25 ;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user,items in user_cmds.items():\n",
    "    if user == getpass.getuser():\n",
    "        for cmd in items:\n",
    "            print(cmd)\n",
    "            ! {cmd}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(user_cmds['nielsond'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for user,items in user_cmds.items():\n",
    "    if user == getpass.getuser():\n",
    "        for cmd in items[1:]:\n",
    "            notsubmitted = True\n",
    "            while notsubmitted:\n",
    "                print(cmd)\n",
    "                !{cmd}\n",
    "                if _exit_code == 0:\n",
    "                    notsubmitted=False\n",
    "                else:\n",
    "                    print(\"wait an hour and then try again\")\n",
    "                    time.sleep(3600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep for multi user run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users=['nielsond', 'adamt']\n",
    "user_cmds = {u:[] for u in users}\n",
    "users = cycle(users)\n",
    "pstep = 50\n",
    "pranges = [(0, 50),\n",
    "           (50,100),\n",
    "           (100,150),\n",
    "           (150,200)]\n",
    "i = 0\n",
    "loop_n = 4\n",
    "swarm_files = []\n",
    "jobs_per_swarm = 1000\n",
    "#modality = 'smri'\n",
    "modalities=['smri', 'dmri', 'tfmri']\n",
    "\n",
    "crt_combinations = longtbl.query('modality == \"tfmri\" & metric == \"beta\" & run != \"all\"').groupby(['task','contrast','run']).size().reset_index().drop(0,axis=1).to_records(index=False)\n",
    "for pr in pranges:\n",
    "    for modality in modalities:\n",
    "        user = next(users)\n",
    "        if modality != 'tfmri':\n",
    "            cmds = make_cmd_list(user, pr, modality, longtbl.query('modality == @modality').metric.unique(), bundle=1,\n",
    "                                out_base='/data/MLDSST/abcd/',\n",
    "                                swarm_dir='/data/MLDSST/abcd/abcd_swarm_dir')\n",
    "        else:\n",
    "            cmds=[]\n",
    "            for task, contrast, run in crt_combinations:\n",
    "                cmds.extend(make_cmd_list(user, pr, modality, 'beta', bundle=2, run=run, task=task, contrast=contrast, out_base='/data/MLDSST/abcd/',\n",
    "                                swarm_dir='/data/MLDSST/abcd/abcd_swarm_dir'))\n",
    "            \n",
    "        for ii in range(0, len(cmds), jobs_per_swarm):\n",
    "            swarm_file = Path('/data/MLDSST/abcd/abcd_swarm_dir/swarm_test_%s_%d_%d_%s_%d_11'%(modality, pr[0],ii,user, loop_n))\n",
    "            swarm_files.append(swarm_file.as_posix())\n",
    "            swarm_file.write_text('\\n'.join(cmds[ii:ii+jobs_per_swarm]))\n",
    "            swarm_cmd = f'swarm -f {swarm_file} -g 12 -t auto --logdir ${{USER}}_swarm_log --time 2:30:00 --partition=quick,norm --gres=lscratch:30 --module singularity --exclusive --sbatch \"--constraint=core16\"'\n",
    "            user_cmds[user].append(swarm_cmd)\n",
    "            print(len(cmds[ii:ii+jobs_per_swarm]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_cmds['adamt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{user_cmds['nielsond'][0]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{user_cmds['nielsond'][1]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_cmds['nielsond']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "for user,items in user_cmds.items():\n",
    "    if user == getpass.getuser():\n",
    "        for cmd in items[2:]:\n",
    "            notsubmitted = True\n",
    "            while notsubmitted:\n",
    "                print(cmd)\n",
    "                !{cmd}\n",
    "                if _exit_code == 0:\n",
    "                    notsubmitted=False\n",
    "                else:\n",
    "                    print(\"wait an hour and then try again\")\n",
    "                    time.sleep(3600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0: 15980554, 1: 15985228"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_cmds = {}\n",
    "user_cmds['adamt'] = ['swarm -f /data/MLDSST/abcd/abcd_swarm_dir/swarm_test_dmri_0_0_adamt_4_11 -g 12 -t auto --logdir ${USER}_swarm_log --time 2:30:00 --partition=quick,norm --gres=lscratch:30 --module singularity --exclusive --sbatch \"--constraint=core16\"',\n",
    "                     'swarm -f /data/MLDSST/abcd/abcd_swarm_dir/swarm_test_dmri_0_1000_adamt_4_11 -g 12 -t auto --logdir ${USER}_swarm_log --time 2:30:00 --partition=quick,norm --gres=lscratch:30 --module singularity --exclusive --sbatch \"--constraint=core16\"',\n",
    "                     'swarm -f /data/MLDSST/abcd/abcd_swarm_dir/swarm_test_dmri_0_2000_adamt_4_11 -g 12 -t auto --logdir ${USER}_swarm_log --time 2:30:00 --partition=quick,norm --gres=lscratch:30 --module singularity --exclusive --sbatch \"--constraint=core16\"',\n",
    "                     'swarm -f /data/MLDSST/abcd/abcd_swarm_dir/swarm_test_smri_50_0_adamt_4_11 -g 12 -t auto --logdir ${USER}_swarm_log --time 2:30:00 --partition=quick,norm --gres=lscratch:30 --module singularity --exclusive --sbatch \"--constraint=core16\"',\n",
    "                     'swarm -f /data/MLDSST/abcd/abcd_swarm_dir/swarm_test_tfmri_50_0_adamt_4_11 -g 12 -t auto --logdir ${USER}_swarm_log --time 2:30:00 --partition=quick,norm --gres=lscratch:30 --module singularity --exclusive --sbatch \"--constraint=core16\"',\n",
    "                     'swarm -f /data/MLDSST/abcd/abcd_swarm_dir/swarm_test_tfmri_50_1000_adamt_4_11 -g 12 -t auto --logdir ${USER}_swarm_log --time 2:30:00 --partition=quick,norm --gres=lscratch:30 --module singularity --exclusive --sbatch \"--constraint=core16\"',\n",
    "                     'swarm -f /data/MLDSST/abcd/abcd_swarm_dir/swarm_test_tfmri_50_2000_adamt_4_11 -g 12 -t auto --logdir ${USER}_swarm_log --time 2:30:00 --partition=quick,norm --gres=lscratch:30 --module singularity --exclusive --sbatch \"--constraint=core16\"',\n",
    "                     'swarm -f /data/MLDSST/abcd/abcd_swarm_dir/swarm_test_tfmri_50_3000_adamt_4_11 -g 12 -t auto --logdir ${USER}_swarm_log --time 2:30:00 --partition=quick,norm --gres=lscratch:30 --module singularity --exclusive --sbatch \"--constraint=core16\"',\n",
    "                     'swarm -f /data/MLDSST/abcd/abcd_swarm_dir/swarm_test_tfmri_50_4000_adamt_4_11 -g 12 -t auto --logdir ${USER}_swarm_log --time 2:30:00 --partition=quick,norm --gres=lscratch:30 --module singularity --exclusive --sbatch \"--constraint=core16\"',\n",
    "                     'swarm -f /data/MLDSST/abcd/abcd_swarm_dir/swarm_test_tfmri_50_5000_adamt_4_11 -g 12 -t auto --logdir ${USER}_swarm_log --time 2:30:00 --partition=quick,norm --gres=lscratch:30 --module singularity --exclusive --sbatch \"--constraint=core16\"',\n",
    "                     'swarm -f /data/MLDSST/abcd/abcd_swarm_dir/swarm_test_dmri_100_0_adamt_4_11 -g 12 -t auto --logdir ${USER}_swarm_log --time 2:30:00 --partition=quick,norm --gres=lscratch:30 --module singularity --exclusive --sbatch \"--constraint=core16\"',\n",
    "                     'swarm -f /data/MLDSST/abcd/abcd_swarm_dir/swarm_test_dmri_100_1000_adamt_4_11 -g 12 -t auto --logdir ${USER}_swarm_log --time 2:30:00 --partition=quick,norm --gres=lscratch:30 --module singularity --exclusive --sbatch \"--constraint=core16\"',\n",
    "                     'swarm -f /data/MLDSST/abcd/abcd_swarm_dir/swarm_test_dmri_100_2000_adamt_4_11 -g 12 -t auto --logdir ${USER}_swarm_log --time 2:30:00 --partition=quick,norm --gres=lscratch:30 --module singularity --exclusive --sbatch \"--constraint=core16\"',\n",
    "                     'swarm -f /data/MLDSST/abcd/abcd_swarm_dir/swarm_test_smri_150_0_adamt_4_11 -g 12 -t auto --logdir ${USER}_swarm_log --time 2:30:00 --partition=quick,norm --gres=lscratch:30 --module singularity --exclusive --sbatch \"--constraint=core16\"',\n",
    "                     'swarm -f /data/MLDSST/abcd/abcd_swarm_dir/swarm_test_tfmri_150_0_adamt_4_11 -g 12 -t auto --logdir ${USER}_swarm_log --time 2:30:00 --partition=quick,norm --gres=lscratch:30 --module singularity --exclusive --sbatch \"--constraint=core16\"',\n",
    "                     'swarm -f /data/MLDSST/abcd/abcd_swarm_dir/swarm_test_tfmri_150_1000_adamt_4_11 -g 12 -t auto --logdir ${USER}_swarm_log --time 2:30:00 --partition=quick,norm --gres=lscratch:30 --module singularity --exclusive --sbatch \"--constraint=core16\"',\n",
    "                     'swarm -f /data/MLDSST/abcd/abcd_swarm_dir/swarm_test_tfmri_150_2000_adamt_4_11 -g 12 -t auto --logdir ${USER}_swarm_log --time 2:30:00 --partition=quick,norm --gres=lscratch:30 --module singularity --exclusive --sbatch \"--constraint=core16\"',\n",
    "                     'swarm -f /data/MLDSST/abcd/abcd_swarm_dir/swarm_test_tfmri_150_3000_adamt_4_11 -g 12 -t auto --logdir ${USER}_swarm_log --time 2:30:00 --partition=quick,norm --gres=lscratch:30 --module singularity --exclusive --sbatch \"--constraint=core16\"',\n",
    "                     'swarm -f /data/MLDSST/abcd/abcd_swarm_dir/swarm_test_tfmri_150_4000_adamt_4_11 -g 12 -t auto --logdir ${USER}_swarm_log --time 2:30:00 --partition=quick,norm --gres=lscratch:30 --module singularity --exclusive --sbatch \"--constraint=core16\"',\n",
    "                     'swarm -f /data/MLDSST/abcd/abcd_swarm_dir/swarm_test_tfmri_150_5000_adamt_4_11 -g 12 -t auto --logdir ${USER}_swarm_log --time 2:30:00 --partition=quick,norm --gres=lscratch:30 --module singularity --exclusive --sbatch \"--constraint=core16\"']\n",
    "\n",
    "import time\n",
    "import getpass\n",
    "for user,items in user_cmds.items():\n",
    "    if user == getpass.getuser():\n",
    "        for cmd in items[1:]:\n",
    "            notsubmitted = True\n",
    "            while notsubmitted:\n",
    "                print(cmd)\n",
    "                !{cmd}\n",
    "                if _exit_code == 0:\n",
    "                    notsubmitted=False\n",
    "                else:\n",
    "                    print(\"wait an hour and then try again\")\n",
    "                    time.sleep(3600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
